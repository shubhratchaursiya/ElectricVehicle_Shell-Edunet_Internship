# WEEK 1 – Electric Vehicle Price Prediction (Model Implementation)
# Author: Shubhrat Chaursiya
# Project: ElectricVehicle_Shell-Edunet_Internship / Week1
# ---------------------------------------------

# ---------------- 1. Import Libraries ----------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ---------------- 2. Load Dataset ----------------
file_path = "Electric_Vehicle_Dataset.csv"  # Update path if necessary
df = pd.read_csv(file_path)

print("Dataset loaded. First five rows:")
print(df.head())

# ---------------- 3. Basic Info ----------------
print("\nShape of dataset:", df.shape)
print("\nData types and info:")
print(df.info())
print("\nStatistical summary (numeric):")
print(df.describe())

# ---------------- 4. Missing Values Check ----------------
print("\nMissing values per column:")
print(df.isnull().sum())

# Example decision: drop rows with many missing values OR impute
# For simplicity, drop rows with any missing value (if few)
df = df.dropna()
print("\nAfter dropping missing rows, new shape:", df.shape)

# ---------------- 5. Exploratory Data Analysis (brief) ----------------
# (You may expand this section as part of Week 1 EDA)
plt.figure(figsize=(8,5))
sns.histplot(df['Price'], kde=True, bins=30)
plt.title('Distribution of EV Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8,6))
corr = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation between numeric features')
plt.show()

# ---------------- 6. Feature Engineering & Encoding ----------------
# Example: drop identifier columns, encode categorical variables
# Drop columns not useful for prediction (update names as per dataset)
cols_to_drop = ['Model_Name', 'Variant'] if {'Model_Name','Variant'}.issubset(df.columns) else []
df = df.drop(columns=cols_to_drop, errors='ignore')

# Encode all categorical columns via one-hot
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
print("Categorical columns:", categorical_cols)
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print("\nEncoded dataset columns count:", len(df_encoded.columns))

# ---------------- 7. Split Dataset ----------------
X = df_encoded.drop('Price', axis=1)
y = df_encoded['Price']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.20,
                                                    random_state=42)
print("\nTraining set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

# ---------------- 8. Feature Scaling ----------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------- 9. Model Training & Hyper-Parameter Tuning ----------------
# Use two candidate models: Ridge regression & Random Forest
print("\nTraining Ridge regression model...")
ridge = Ridge()
param_grid_ridge = {'alpha': [0.1, 1.0, 10.0]}
grid_ridge = GridSearchCV(ridge, param_grid_ridge, cv=5, scoring='r2')
grid_ridge.fit(X_train_scaled, y_train)
print("Best Ridge params:", grid_ridge.best_params_)

print("\nTraining Random Forest regression model...")
rf = RandomForestRegressor(random_state=42)
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_leaf': [1, 2]
}
grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='r2', n_jobs=-1)
grid_rf.fit(X_train, y_train)
print("Best Random Forest params:", grid_rf.best_params_)

# Choose best performing model (based on R² on validation)
best_model = grid_rf.best_estimator_

# ---------------- 10. Model Evaluation ----------------
y_pred = best_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nModel performance on Test Set:")
print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)
print("R² Score:", r2)

# ---------------- 11. Visualization of Predictions ----------------
plt.figure(figsize=(7,5))
plt.scatter(y_test, y_pred, alpha=0.7, color='green')
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted EV Price")
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         color='red', lw=2)
plt.show()

# ---------------- 12. Summary / Save Model ----------------
import joblib
model_filename = "ev_price_model_week1.pkl"
scaler_filename = "scaler_week1.pkl"
joblib.dump(best_model, model_filename)
joblib.dump(scaler, scaler_filename)
print(f"\nModel saved to {model_filename} and scaler to {scaler_filename}")

print("\n✅ Week 1 completed: Model training and evaluation done.")
